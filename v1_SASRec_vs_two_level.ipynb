{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1ylh8LzEv6cz1UKgG3h4985r1o3apEdT5",
      "authorship_tag": "ABX9TyPSxgKyHuUwB30n8qGLRf7q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/budennovsk/AuthorBooksComments/blob/master/v1_SASRec_vs_two_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install implicit catboost rectools[lightfm] replay-rec==0.21.2rc0 #replay-rec\n",
        "# !pip -q uninstall -y pyspark\n",
        "# !pip -q install \"pyspark==3.4.3\"\n",
        "import sys\n",
        "import pyspark\n",
        "print(\"python:\", sys.version)\n",
        "print(\"pyspark:\", pyspark.__version__)"
      ],
      "metadata": {
        "id": "DU66BHTGShrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d46GbIK-Q5y8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import typing as tp\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from implicit.nearest_neighbours import CosineRecommender\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from catboost import CatBoostClassifier, CatBoostRanker\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier, LGBMRanker\n",
        "    LGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    warnings.warn(\"lightgbm is not installed. Some parts of the notebook will be skipped.\")\n",
        "    LGBM_AVAILABLE = False\n",
        "\n",
        "from lightfm import LightFM\n",
        "from rectools import Columns\n",
        "from rectools.dataset import Dataset\n",
        "from rectools.metrics import Precision, Recall, MeanInvUserFreq, Serendipity,MAP,NDCG,HitRate\n",
        "from rectools.models import (\n",
        "    ImplicitALSWrapperModel,\n",
        "    ImplicitBPRWrapperModel,\n",
        "    LightFMWrapperModel,\n",
        "    PureSVDModel,\n",
        "    ImplicitItemKNNWrapperModel,\n",
        "    EASEModel,\n",
        "    PopularModel)\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from implicit.bpr import BayesianPersonalizedRanking\n",
        "from implicit.nearest_neighbours import CosineRecommender\n",
        "from rectools.models.base import ExternalIds\n",
        "from rectools.models.ranking import (\n",
        "    CandidateRankingModel,\n",
        "    CandidateGenerator,\n",
        "    Reranker,\n",
        "    CatBoostReranker,\n",
        "    CandidateFeatureCollector,\n",
        "    PerUserNegativeSampler\n",
        ")\n",
        "from rectools.model_selection import cross_validate, TimeRangeSplitter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_users = '/content/drive/MyDrive/Colab Notebooks/Симбирсофт/recsys/dataset/data_original/users.csv'\n",
        "path_items = '/content/drive/MyDrive/Colab Notebooks/Симбирсофт/recsys/dataset/data_original/items.csv'\n",
        "path_interactions = '/content/drive/MyDrive/Colab Notebooks/Симбирсофт/recsys/dataset/data_original/interactions.csv'\n",
        "\n",
        "\n",
        "users = pd.read_csv(path_users)\n",
        "items = pd.read_csv(path_items)\n",
        "interactions = (\n",
        "    pd.read_csv(path_interactions, parse_dates=[\"last_watch_dt\"])\n",
        "    .rename(columns={\"last_watch_dt\": Columns.Datetime})\n",
        ")\n",
        "interactions = interactions.head(1000000)\n",
        "interactions[\"weight\"] = 1\n",
        "dataset = Dataset.construct(interactions)\n",
        "RANDOM_STATE = 32\n",
        "dataset"
      ],
      "metadata": {
        "id": "yYk0XhrxWWxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import typing as tp\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LogStatFeaturesProcessorPandas:\n",
        "    \"\"\"\n",
        "    Pandas-версия Spark-класса LogStatFeaturesProcessor.\n",
        "\n",
        "    Входной лог: pandas.DataFrame со столбцами:\n",
        "      - user_idx (int/str)\n",
        "      - item_idx (int/str)\n",
        "      - relevance (float/int)  (если нет — можно заранее создать relevance=1.0)\n",
        "      - timestamp (datetime64[ns], опционально)\n",
        "\n",
        "    Что считает (по аналогии со Spark-версией):\n",
        "    - log_num_interact: log(count(relevance)) по user и item\n",
        "    - timestamp-based (если timestamp есть и >1 уникального):\n",
        "        * log_interact_days_count = log(#уникальных дней взаимодействий)\n",
        "        * min/max interact date\n",
        "        * history_length_days = (max - min) в днях\n",
        "        * last_interaction_gap_days = (max_log_date - max) в днях\n",
        "    - relevance-based (если relevance имеет >1 уникального значения):\n",
        "        * mean/std (std NaN -> 0)\n",
        "        * приблизительные квантили 0.05/0.5/0.95\n",
        "        * abnormality и abnormalityCR на пользователя\n",
        "    - cross features:\n",
        "        * i_mean_u_log_num_interact: для item средний u_log_num_interact по пользователям, которые с ним взаимодействовали\n",
        "        * u_mean_i_log_num_interact: для user средний i_log_num_interact по item, с которыми он взаимодействовал\n",
        "    - transform:\n",
        "        * джойнит user и item фичи в лог\n",
        "        * na_u_log_features / na_i_log_features\n",
        "        * u_i_log_num_interact_diff / i_u_log_num_interact_diff\n",
        "    \"\"\"\n",
        "\n",
        "    user_log_features: tp.Optional[pd.DataFrame] = None\n",
        "    item_log_features: tp.Optional[pd.DataFrame] = None\n",
        "    calc_timestamp_based: bool = False\n",
        "    calc_relevance_based: bool = False\n",
        "    max_log_date: tp.Optional[pd.Timestamp] = None\n",
        "\n",
        "    def fit(self, log: pd.DataFrame) -> \"LogStatFeaturesProcessorPandas\":\n",
        "        req_cols = {\"user_idx\", \"item_idx\", \"relevance\"}\n",
        "        missing = req_cols - set(log.columns)\n",
        "        if missing:\n",
        "            raise ValueError(f\"В логе нет обязательных колонок {missing}. Нужно минимум: {sorted(req_cols)}\")\n",
        "\n",
        "        df = log.copy()\n",
        "\n",
        "        # Определяем, можно ли считать timestamp-based\n",
        "        if \"timestamp\" in df.columns:\n",
        "            # приводим к datetime\n",
        "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "            ts_unique = df[\"timestamp\"].nunique(dropna=True)\n",
        "            self.calc_timestamp_based = (ts_unique is not None) and (ts_unique > 1) and df[\"timestamp\"].notna().any()\n",
        "        else:\n",
        "            self.calc_timestamp_based = False\n",
        "\n",
        "        # Определяем, можно ли считать relevance-based\n",
        "        rel_unique = df[\"relevance\"].nunique(dropna=True)\n",
        "        self.calc_relevance_based = (rel_unique is not None) and (rel_unique > 1)\n",
        "\n",
        "        if self.calc_timestamp_based:\n",
        "            self.max_log_date = pd.to_datetime(df[\"timestamp\"].max())\n",
        "        else:\n",
        "            self.max_log_date = None\n",
        "\n",
        "        # ----------------------------\n",
        "        # 1) Базовые user агрегации\n",
        "        # ----------------------------\n",
        "        # log(count(relevance))\n",
        "        user_grp = df.groupby(\"user_idx\", dropna=False)\n",
        "\n",
        "        user_features = user_grp.agg(\n",
        "            u_num_interact=(\"relevance\", \"count\"),\n",
        "        )\n",
        "        user_features[\"u_log_num_interact\"] = np.log(user_features[\"u_num_interact\"].astype(float).clip(lower=1.0))\n",
        "        user_features = user_features.drop(columns=[\"u_num_interact\"])\n",
        "\n",
        "        # timestamp-based user\n",
        "        if self.calc_timestamp_based:\n",
        "            # уникальные дни\n",
        "            df[\"_date\"] = df[\"timestamp\"].dt.floor(\"D\")\n",
        "            user_days = df.groupby(\"user_idx\")[\"_date\"].nunique()\n",
        "            user_features[\"u_log_interact_days_count\"] = np.log(user_days.astype(float).clip(lower=1.0))\n",
        "\n",
        "            u_min = df.groupby(\"user_idx\")[\"timestamp\"].min()\n",
        "            u_max = df.groupby(\"user_idx\")[\"timestamp\"].max()\n",
        "            user_features[\"u_min_interact_date\"] = u_min\n",
        "            user_features[\"u_max_interact_date\"] = u_max\n",
        "\n",
        "            user_features[\"u_history_length_days\"] = (\n",
        "                (user_features[\"u_max_interact_date\"] - user_features[\"u_min_interact_date\"])\n",
        "                .dt.total_seconds()\n",
        "                .div(86400.0)\n",
        "                .fillna(0.0)\n",
        "            )\n",
        "\n",
        "            user_features[\"u_last_interaction_gap_days\"] = (\n",
        "                (self.max_log_date - user_features[\"u_max_interact_date\"])\n",
        "                .dt.total_seconds()\n",
        "                .div(86400.0)\n",
        "                .fillna(0.0)\n",
        "            )\n",
        "\n",
        "        # relevance-based user (mean/std/quantiles)\n",
        "        if self.calc_relevance_based:\n",
        "            u_mean = user_grp[\"relevance\"].mean()\n",
        "            u_std = user_grp[\"relevance\"].std(ddof=1).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            user_features[\"u_mean\"] = u_mean\n",
        "            user_features[\"u_std\"] = u_std\n",
        "\n",
        "            # квантили (как приближение percentile_approx)\n",
        "            user_features[\"u_quantile_05\"] = user_grp[\"relevance\"].quantile(0.05, interpolation=\"linear\")\n",
        "            user_features[\"u_quantile_50\"] = user_grp[\"relevance\"].quantile(0.50, interpolation=\"linear\")\n",
        "            user_features[\"u_quantile_95\"] = user_grp[\"relevance\"].quantile(0.95, interpolation=\"linear\")\n",
        "\n",
        "        user_features = user_features.reset_index()\n",
        "\n",
        "        # ----------------------------\n",
        "        # 2) Базовые item агрегации\n",
        "        # ----------------------------\n",
        "        item_grp = df.groupby(\"item_idx\", dropna=False)\n",
        "\n",
        "        item_features = item_grp.agg(\n",
        "            i_num_interact=(\"relevance\", \"count\"),\n",
        "        )\n",
        "        item_features[\"i_log_num_interact\"] = np.log(item_features[\"i_num_interact\"].astype(float).clip(lower=1.0))\n",
        "        item_features = item_features.drop(columns=[\"i_num_interact\"])\n",
        "\n",
        "        if self.calc_timestamp_based:\n",
        "            # df[\"_date\"] уже есть, если timestamp-based\n",
        "            item_days = df.groupby(\"item_idx\")[\"_date\"].nunique()\n",
        "            item_features[\"i_log_interact_days_count\"] = np.log(item_days.astype(float).clip(lower=1.0))\n",
        "\n",
        "            i_min = df.groupby(\"item_idx\")[\"timestamp\"].min()\n",
        "            i_max = df.groupby(\"item_idx\")[\"timestamp\"].max()\n",
        "            item_features[\"i_min_interact_date\"] = i_min\n",
        "            item_features[\"i_max_interact_date\"] = i_max\n",
        "\n",
        "            item_features[\"i_history_length_days\"] = (\n",
        "                (item_features[\"i_max_interact_date\"] - item_features[\"i_min_interact_date\"])\n",
        "                .dt.total_seconds()\n",
        "                .div(86400.0)\n",
        "                .fillna(0.0)\n",
        "            )\n",
        "\n",
        "            item_features[\"i_last_interaction_gap_days\"] = (\n",
        "                (self.max_log_date - item_features[\"i_max_interact_date\"])\n",
        "                .dt.total_seconds()\n",
        "                .div(86400.0)\n",
        "                .fillna(0.0)\n",
        "            )\n",
        "\n",
        "        if self.calc_relevance_based:\n",
        "            i_mean = item_grp[\"relevance\"].mean()\n",
        "            i_std = item_grp[\"relevance\"].std(ddof=1).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "            item_features[\"i_mean\"] = i_mean\n",
        "            item_features[\"i_std\"] = i_std\n",
        "\n",
        "            item_features[\"i_quantile_05\"] = item_grp[\"relevance\"].quantile(0.05, interpolation=\"linear\")\n",
        "            item_features[\"i_quantile_50\"] = item_grp[\"relevance\"].quantile(0.50, interpolation=\"linear\")\n",
        "            item_features[\"i_quantile_95\"] = item_grp[\"relevance\"].quantile(0.95, interpolation=\"linear\")\n",
        "\n",
        "        item_features = item_features.reset_index()\n",
        "\n",
        "        # ----------------------------\n",
        "        # 3) Abnormality (по статье) — на пользователя, используя item mean/std\n",
        "        # ----------------------------\n",
        "        if self.calc_relevance_based:\n",
        "            tmp = df.merge(item_features[[\"item_idx\", \"i_mean\", \"i_std\"]], on=\"item_idx\", how=\"left\")\n",
        "\n",
        "            tmp[\"abnormality\"] = (tmp[\"relevance\"] - tmp[\"i_mean\"]).abs()\n",
        "\n",
        "            abn_user = tmp.groupby(\"user_idx\")[\"abnormality\"].mean().rename(\"abnormality\")\n",
        "\n",
        "            # AbnormalityCR\n",
        "            max_std = float(item_features[\"i_std\"].max())\n",
        "            min_std = float(item_features[\"i_std\"].min())\n",
        "\n",
        "            abn_user_df = abn_user.to_frame()\n",
        "\n",
        "            if (max_std - min_std) != 0:\n",
        "                tmp[\"controversy\"] = 1.0 - (tmp[\"i_std\"] - min_std) / (max_std - min_std)\n",
        "                tmp[\"abnormalityCR\"] = (tmp[\"abnormality\"] * tmp[\"controversy\"]) ** 2\n",
        "                abncr_user = tmp.groupby(\"user_idx\")[\"abnormalityCR\"].mean().rename(\"abnormalityCR\")\n",
        "                abn_user_df = abn_user_df.join(abncr_user, how=\"left\")\n",
        "            else:\n",
        "                abn_user_df[\"abnormalityCR\"] = 0.0\n",
        "\n",
        "            abn_user_df = abn_user_df.reset_index()\n",
        "\n",
        "            user_features = user_features.merge(abn_user_df, on=\"user_idx\", how=\"left\")\n",
        "\n",
        "        # ----------------------------\n",
        "        # 4) Cross interactions counts (средние лог-счётчики \"по другой стороне\")\n",
        "        # ----------------------------\n",
        "        # i_mean_u_log_num_interact: для item средний u_log_num_interact по пользователям, кто с ним взаимодействовал\n",
        "        tmp_i = df[[\"user_idx\", \"item_idx\"]].merge(\n",
        "            user_features[[\"user_idx\", \"u_log_num_interact\"]],\n",
        "            on=\"user_idx\",\n",
        "            how=\"left\",\n",
        "        )\n",
        "        i_cross = tmp_i.groupby(\"item_idx\")[\"u_log_num_interact\"].mean().rename(\"i_mean_u_log_num_interact\").reset_index()\n",
        "\n",
        "        # u_mean_i_log_num_interact: для user средний i_log_num_interact по item, с которыми он взаимодействовал\n",
        "        tmp_u = df[[\"user_idx\", \"item_idx\"]].merge(\n",
        "            item_features[[\"item_idx\", \"i_log_num_interact\"]],\n",
        "            on=\"item_idx\",\n",
        "            how=\"left\",\n",
        "        )\n",
        "        u_cross = tmp_u.groupby(\"user_idx\")[\"i_log_num_interact\"].mean().rename(\"u_mean_i_log_num_interact\").reset_index()\n",
        "\n",
        "        user_features = user_features.merge(u_cross, on=\"user_idx\", how=\"left\")\n",
        "        item_features = item_features.merge(i_cross, on=\"item_idx\", how=\"left\")\n",
        "\n",
        "        # Заполняем пропуски 0 (как fillna в Spark)\n",
        "        user_features = user_features.fillna(0)\n",
        "        item_features = item_features.fillna(0)\n",
        "\n",
        "        self.user_log_features = user_features\n",
        "        self.item_log_features = item_features\n",
        "        return self\n",
        "\n",
        "    def transform(self, log: pd.DataFrame) -> pd.DataFrame:\n",
        "        if self.user_log_features is None or self.item_log_features is None:\n",
        "            raise RuntimeError(\"Сначала вызови fit(log), потом transform(log).\")\n",
        "\n",
        "        df = log.copy()\n",
        "\n",
        "        joined = (\n",
        "            df.merge(self.user_log_features, on=\"user_idx\", how=\"left\", suffixes=(\"\", \"\"))\n",
        "              .merge(self.item_log_features, on=\"item_idx\", how=\"left\", suffixes=(\"\", \"\"))\n",
        "        )\n",
        "\n",
        "        joined[\"na_u_log_features\"] = np.where(joined[\"u_log_num_interact\"].isna(), 1.0, 0.0)\n",
        "        joined[\"na_i_log_features\"] = np.where(joined[\"i_log_num_interact\"].isna(), 1.0, 0.0)\n",
        "\n",
        "        # как Spark fillna(..., 0)\n",
        "        joined = joined.fillna(0)\n",
        "\n",
        "        joined[\"u_i_log_num_interact_diff\"] = joined[\"u_log_num_interact\"] - joined[\"i_mean_u_log_num_interact\"]\n",
        "        joined[\"i_u_log_num_interact_diff\"] = joined[\"i_log_num_interact\"] - joined[\"u_mean_i_log_num_interact\"]\n",
        "\n",
        "        return joined"
      ],
      "metadata": {
        "id": "34rnze-33mtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# 1) Приводим к формату процессора\n",
        "log = interactions.copy()\n",
        "\n",
        "# Подстрой эти rename под свои реальные названия:\n",
        "# часто у тебя: user_id, item_id (или movie_id), weight, timestamp\n",
        "rename_map = {}\n",
        "if \"user_id\" in log.columns:\n",
        "    rename_map[\"user_id\"] = \"user_idx\"\n",
        "if \"item_id\" in log.columns:\n",
        "    rename_map[\"item_id\"] = \"item_idx\"\n",
        "if \"movie_id\" in log.columns and \"item_id\" not in log.columns:\n",
        "    rename_map[\"movie_id\"] = \"item_idx\"\n",
        "if \"watched_pct\" in log.columns and \"relevance\" not in log.columns:\n",
        "    rename_map[\"watched_pct\"] = \"relevance\"\n",
        "\n",
        "log = log.rename(columns=rename_map)\n",
        "\n",
        "# Если relevance так и нет — сделаем implicit = 1.0\n",
        "if \"relevance\" not in log.columns:\n",
        "    log[\"relevance\"] = 1.0\n",
        "\n",
        "# Если timestamp есть, приведём к datetime (процессор тоже пытается, но так надёжнее)\n",
        "if \"timestamp\" in log.columns:\n",
        "    log[\"timestamp\"] = pd.to_datetime(log[\"timestamp\"], errors=\"coerce\")\n",
        "\n",
        "# Оставим только нужное (чтобы случайные колонки не мешали)\n",
        "keep_cols = [c for c in [\"user_idx\", \"item_idx\", \"timestamp\", \"relevance\"] if c in log.columns]\n",
        "log = log[keep_cols].copy()\n",
        "\n",
        "\n",
        "\n",
        "# 2) Fit + Transform\n",
        "proc = LogStatFeaturesProcessorPandas()\n",
        "proc.fit(log)\n",
        "\n",
        "log_with_features = proc.transform(log[[\"user_idx\", \"item_idx\"]].copy())\n",
        "log_with_features"
      ],
      "metadata": {
        "id": "auofxXXF4VTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log"
      ],
      "metadata": {
        "id": "co7Zprto5bMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactions"
      ],
      "metadata": {
        "id": "HUnvF86M9po5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from replay.preprocessing.history_based_fp import LogStatFeaturesProcessor\n",
        "from replay.utils.session_handler import get_spark_session, State\n",
        "from replay.experimental.preprocessing.data_preparator import DataPreparator, Indexer\n",
        "spark = State().session\n",
        "spark.sparkContext.setLogLevel('ERROR')\n",
        "dp = DataPreparator()\n",
        "log = dp.transform(data=interactions,\n",
        "                  columns_mapping={\n",
        "                      \"user_id\": \"user_id\",\n",
        "                      \"item_id\":  \"item_id\",\n",
        "                      \"relevance\": \"watched_pct\",\n",
        "                      \"timestamp\": \"datetime\"\n",
        "                  })\n",
        "\n",
        "log.show(2)"
      ],
      "metadata": {
        "id": "oHkq9DDdxfp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = Indexer(user_col='user_id', item_col='item_id')\n",
        "indexer.fit(users=log.select('user_id'),\n",
        "            items=log.select('item_id'))\n",
        "log = indexer.transform(df=log)\n",
        "log.show(2)"
      ],
      "metadata": {
        "id": "AFzFWc23-CU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del indexer"
      ],
      "metadata": {
        "id": "fXDuK47XNaRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from replay.preprocessing.history_based_fp import LogStatFeaturesProcessor\n",
        "lf = LogStatFeaturesProcessor()\n",
        "lf.fit(log)\n",
        "# log_trsfrm = lf.transform(log)\n",
        "# log_trsfrm.show(1, vertical=True)"
      ],
      "metadata": {
        "id": "Kv5xCnVg-CXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_trsfrm = lf.transform(log)\n"
      ],
      "metadata": {
        "id": "9bOcftl_P7BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_trsfrm.show(1, vertical=True)"
      ],
      "metadata": {
        "id": "l63c31Rl-CZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_pd = log_trsfrm.toPandas()\n",
        "log_pd.head()"
      ],
      "metadata": {
        "id": "OJJ3cAisRMnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_pd[['item_id', 'user_id', 'datetime', 'total_dur', 'watched_pct', 'weight',\n",
        "       'u_log_num_interact', 'u_log_interact_days_count',\n",
        "       'u_min_interact_date', 'u_max_interact_date', 'u_std', 'u_mean',\n",
        "       'u_quantile_05', 'u_quantile_5', 'u_quantile_95',\n",
        "       'u_history_length_days', 'u_last_interaction_gap_days', 'abnormality',\n",
        "       'abnormalityCR', 'u_mean_i_log_num_interact', 'i_log_num_interact',\n",
        "       'i_log_interact_days_count', 'i_min_interact_date',\n",
        "       'i_max_interact_date', 'i_std', 'i_mean', 'i_quantile_05',\n",
        "       'i_quantile_5', 'i_quantile_95', 'i_history_length_days',\n",
        "       'i_last_interaction_gap_days', 'i_mean_u_log_num_interact',\n",
        "       'na_u_log_features', 'na_i_log_features', 'u_i_log_num_interact_diff',\n",
        "       'i_u_log_num_interact_diff']]"
      ],
      "metadata": {
        "id": "3z7N2M9dZEwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare first stage models. They will be used to generate candidates for reranking\n",
        "first_stage = [\n",
        "    CandidateGenerator(PopularModel(), num_candidates=100, keep_ranks=True, keep_scores=True),\n",
        "    CandidateGenerator(\n",
        "        ImplicitItemKNNWrapperModel(CosineRecommender()),\n",
        "        num_candidates=100,\n",
        "        keep_ranks=True,\n",
        "        keep_scores=True\n",
        "    ),\n",
        "    CandidateGenerator(\n",
        "        ImplicitALSWrapperModel(\n",
        "          AlternatingLeastSquares(\n",
        "            factors=10,  # latent embeddings size\n",
        "            regularization=0.1,\n",
        "            iterations=10,\n",
        "            alpha=50,  # confidence multiplier for non-zero entries in interactions\n",
        "            random_state=RANDOM_STATE)),\n",
        "    num_candidates=100, keep_ranks=True, keep_scores=True),\n",
        "    CandidateGenerator(\n",
        "        LightFMWrapperModel(\n",
        "            LightFM(no_components=10,\n",
        "                    loss=\"bpr\",\n",
        "                    random_state=RANDOM_STATE)),\n",
        "    num_candidates=100, keep_ranks=True, keep_scores=True\n",
        ")\n",
        "]"
      ],
      "metadata": {
        "id": "PKNU4EvuaVfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomFeatureCollector(CandidateFeatureCollector):\n",
        "\n",
        "#     def __init__(self, user_features_path: Path, user_cat_cols: tp.List[str]) -> None:\n",
        "#         self.user_features_path = user_features_path\n",
        "#         self.user_cat_cols = user_cat_cols\n",
        "\n",
        "#     # your any helper functions for working with loaded data\n",
        "#     def _encode_cat_cols(self, df: pd.DataFrame, cols: tp.List[str]) -> pd.DataFrame:\n",
        "#         for col in cols:\n",
        "#             df[col] = df[col].astype(\"category\").cat.codes.astype(\"category\")\n",
        "#         return df\n",
        "\n",
        "#     def _get_user_features(\n",
        "#         self, users: ExternalIds, dataset: Dataset, fold_info: tp.Optional[tp.Dict[str, tp.Any]]\n",
        "#     ) -> pd.DataFrame:\n",
        "#         columns = self.user_cat_cols.copy()\n",
        "#         columns.append(Columns.User)\n",
        "#         user_features = pd.read_csv(self.user_features_path)[columns]\n",
        "\n",
        "#         users_without_features = pd.DataFrame(\n",
        "#             np.setdiff1d(dataset.user_id_map.external_ids, user_features[Columns.User].unique()),\n",
        "#             columns=[Columns.User]\n",
        "#         )\n",
        "#         user_features = pd.concat([user_features, users_without_features], axis=0)\n",
        "#         user_features = self._encode_cat_cols(user_features, self.user_cat_cols)\n",
        "\n",
        "#         return user_features[user_features[Columns.User].isin(users)]"
      ],
      "metadata": {
        "id": "LO8CtVoFieZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import typing as tp\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from rectools import Columns\n",
        "from rectools.dataset import Dataset\n",
        "from rectools.models.ranking import CandidateFeatureCollector  # если у тебя импорт другой — оставь как было\n",
        "\n",
        "ExternalIds = tp.Iterable[tp.Any]\n",
        "\n",
        "\n",
        "class CustomFeatureCollector(CandidateFeatureCollector):\n",
        "    def __init__(\n",
        "        self,\n",
        "        user_features_path: Path,\n",
        "        user_cat_cols: tp.List[str],\n",
        "        pair_features: pd.DataFrame,  # <-- добавили\n",
        "    ) -> None:\n",
        "        self.user_features_path = user_features_path\n",
        "        self.user_cat_cols = user_cat_cols\n",
        "        self.pair_features = pair_features\n",
        "\n",
        "    def _encode_cat_cols(self, df: pd.DataFrame, cols: tp.List[str]) -> pd.DataFrame:\n",
        "        for col in cols:\n",
        "            df[col] = df[col].astype(\"category\").cat.codes.astype(\"category\")\n",
        "        return df\n",
        "\n",
        "    def _get_user_features(\n",
        "        self, users: ExternalIds, dataset: Dataset, fold_info: tp.Optional[tp.Dict[str, tp.Any]]\n",
        "    ) -> pd.DataFrame:\n",
        "        columns = self.user_cat_cols.copy()\n",
        "        columns.append(Columns.User)\n",
        "        user_features = pd.read_csv(self.user_features_path)[columns]\n",
        "\n",
        "        users_without_features = pd.DataFrame(\n",
        "            np.setdiff1d(dataset.user_id_map.external_ids, user_features[Columns.User].unique()),\n",
        "            columns=[Columns.User],\n",
        "        )\n",
        "        user_features = pd.concat([user_features, users_without_features], axis=0)\n",
        "        user_features = self._encode_cat_cols(user_features, self.user_cat_cols)\n",
        "\n",
        "        return user_features[user_features[Columns.User].isin(users)]\n",
        "\n",
        "    def _get_pair_features(\n",
        "        self, pairs: pd.DataFrame\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        pairs: DataFrame со столбцами [user_id, item_id] (то есть [Columns.User, Columns.Item])\n",
        "        возвращает pair-features только для нужных пар\n",
        "        \"\"\"\n",
        "        # Оставим только те пары, которые нужны для текущих candidates\n",
        "        pf = pairs.merge(self.pair_features, on=[Columns.User, Columns.Item], how=\"left\")\n",
        "\n",
        "        # Заполним NaN нулями по всем добавленным фичам\n",
        "        added_cols = [c for c in self.pair_features.columns if c not in (Columns.User, Columns.Item)]\n",
        "        pf[added_cols] = pf[added_cols].fillna(0)\n",
        "\n",
        "        return pf\n",
        "\n",
        "    def collect(  # <-- если у тебя метод называется иначе, скажи/пришли ошибку - переименуем\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        candidates: pd.DataFrame,\n",
        "        fold_info: tp.Optional[tp.Dict[str, tp.Any]] = None,\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        candidates обычно содержит как минимум Columns.User и Columns.Item (+ score/target/etc)\n",
        "        Возвращаем датафрейм с признаками для reranker (CatBoost).\n",
        "        \"\"\"\n",
        "        # 1) user-features\n",
        "        users = candidates[Columns.User].unique()\n",
        "        uf = self._get_user_features(users=users, dataset=dataset, fold_info=fold_info)\n",
        "\n",
        "        # 2) pair-features (на конкретные пары candidates)\n",
        "        pairs = candidates[[Columns.User, Columns.Item]].drop_duplicates()\n",
        "        pf = self._get_pair_features(pairs)\n",
        "\n",
        "        # 3) join: candidates + user_features + pair_features\n",
        "        out = (\n",
        "            candidates\n",
        "            .merge(uf, on=Columns.User, how=\"left\")\n",
        "            .merge(pf, on=[Columns.User, Columns.Item], how=\"left\")\n",
        "        )\n",
        "\n",
        "        # user фичи для отсутствующих пользователей\n",
        "        out[self.user_cat_cols] = out[self.user_cat_cols].fillna(-1)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "wztEQQPJZ-a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # To transfer CatBoostRanker we use CatBoostReranker\n",
        "# splitter = TimeRangeSplitter(\"7D\", n_splits=1)\n",
        "\n",
        "# # Categorical features are definitely transferred to the pool_kwargs\n",
        "# pool_kwargs = {\n",
        "#     \"cat_features\": [\"age\", \"income\", \"sex\"]\n",
        "# }\n",
        "\n",
        "# two_stage_catboost_ranker = CandidateRankingModel(\n",
        "#     candidate_generators=first_stage,\n",
        "#     splitter=splitter,\n",
        "#     reranker=CatBoostReranker(CatBoostRanker(verbose=False, random_state=RANDOM_STATE), pool_kwargs=pool_kwargs),\n",
        "#     sampler=PerUserNegativeSampler(n_negatives=3, random_state=RANDOM_STATE), # pass sampler to fix random_state\n",
        "#     # feature_collector=CandidateFeatureCollector(),\n",
        "#     feature_collector=CustomFeatureCollector(user_features_path=path_users, user_cat_cols=[\"age\", \"income\", \"sex\"] ),\n",
        "\n",
        "# )"
      ],
      "metadata": {
        "id": "2jOSxTKahrpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pool_kwargs как у тебя\n",
        "pool_kwargs = {\"cat_features\": [\"age\", \"income\", \"sex\"]}\n",
        "\n",
        "\n",
        "two_stage_catboost_ranker = CandidateRankingModel(\n",
        "    candidate_generators=first_stage,\n",
        "    splitter=splitter,\n",
        "    reranker=CatBoostReranker(\n",
        "        CatBoostRanker(verbose=False, random_state=RANDOM_STATE),\n",
        "        pool_kwargs=pool_kwargs\n",
        "    ),\n",
        "    sampler=PerUserNegativeSampler(n_negatives=3, random_state=RANDOM_STATE),\n",
        "    feature_collector=CustomFeatureCollector(\n",
        "        user_features_path=path_users,\n",
        "        user_cat_cols=[\"age\", \"income\", \"sex\"],\n",
        "        pair_features=log_pd[['item_id', 'user_id', 'datetime', 'total_dur', 'watched_pct', 'weight',\n",
        "       'u_log_num_interact', 'u_log_interact_days_count',\n",
        "       'u_min_interact_date', 'u_max_interact_date', 'u_std', 'u_mean',\n",
        "       'u_quantile_05', 'u_quantile_5', 'u_quantile_95',\n",
        "       'u_history_length_days', 'u_last_interaction_gap_days', 'abnormality',\n",
        "       'abnormalityCR', 'u_mean_i_log_num_interact', 'i_log_num_interact',\n",
        "       'i_log_interact_days_count', 'i_min_interact_date',\n",
        "       'i_max_interact_date', 'i_std', 'i_mean', 'i_quantile_05',\n",
        "       'i_quantile_5', 'i_quantile_95', 'i_history_length_days',\n",
        "       'i_last_interaction_gap_days', 'i_mean_u_log_num_interact',\n",
        "       'na_u_log_features', 'na_i_log_features', 'u_i_log_num_interact_diff',\n",
        "       'i_u_log_num_interact_diff']],  # <-- добавили\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "3OgLp1ABaCej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rename_map = {\n",
        "    \"user_idx\": \"user_id\",\n",
        "    \"item_idx\": \"item_id\",\n",
        "    \"timestamp\": \"datetime\",\n",
        "    \"relevance\": \"watched_pct\"}\n",
        "\n",
        "log_pd = log_pd.rename(columns=rename_map)\n",
        "log_pd.head()"
      ],
      "metadata": {
        "id": "sVn8eutbSzyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.construct(log_pd)"
      ],
      "metadata": {
        "id": "JYPqjoZuTvEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = two_stage_catboost_ranker.get_train_with_targets_for_reranker(dataset)"
      ],
      "metadata": {
        "id": "NgfQV2Wqr5a-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidates.head(10)"
      ],
      "metadata": {
        "id": "ATXDzCTYsJ2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_stage_catboost_ranker.fit(dataset)"
      ],
      "metadata": {
        "id": "8zoL7FsaxaL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_users = dataset.user_id_map.external_ids\n",
        "users_to_recommend = all_users[:100]\n",
        "\n",
        "reco_catboost_ranker = two_stage_catboost_ranker.recommend(\n",
        "    users=users_to_recommend,\n",
        "    dataset=dataset,\n",
        "    k=10,\n",
        "    filter_viewed=True\n",
        ")\n",
        "reco_catboost_ranker.head(5)"
      ],
      "metadata": {
        "id": "vfCXY0LwuIxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take few models to compare\n",
        "models = {\n",
        "    \"popular\": PopularModel(),\n",
        "    \"cosine_knn\": ImplicitItemKNNWrapperModel(CosineRecommender()),\n",
        "    'iALS':ImplicitALSWrapperModel(\n",
        "          AlternatingLeastSquares(\n",
        "            factors=10,  # latent embeddings size\n",
        "            regularization=0.1,\n",
        "            iterations=10,\n",
        "            alpha=50,  # confidence multiplier for non-zero entries in interactions\n",
        "            random_state=RANDOM_STATE)),\n",
        "    'LightFM':LightFMWrapperModel(\n",
        "            LightFM(no_components=10,\n",
        "                    loss=\"bpr\",\n",
        "                    random_state=RANDOM_STATE)),\n",
        "    \"two_stage_catboost_ranker\": two_stage_catboost_ranker,\n",
        "}\n",
        "\n",
        "# We will calculate several classic (precision@k and recall@k) and \"beyond accuracy\" metrics\n",
        "metrics = {\n",
        "    \"prec@10\": Precision(k=10),\n",
        "    \"recall@10\": Recall(k=10),\n",
        "    \"novelty@10\": MeanInvUserFreq(k=10),\n",
        "    \"serendipity@10\": Serendipity(k=10),\n",
        "    \"MAP@10\": MAP(k=10),\n",
        "    \"NDCG@10\": NDCG(k=10),\n",
        "    \"HitRate@10\": HitRate(k=10)\n",
        "}\n",
        "\n",
        "K_RECS = 10"
      ],
      "metadata": {
        "id": "mZKMvPCNxqGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_results = cross_validate(\n",
        "    dataset=dataset,\n",
        "    splitter=splitter,\n",
        "    models=models,\n",
        "    metrics=metrics,\n",
        "    k=K_RECS,\n",
        "    filter_viewed=True,\n",
        ")"
      ],
      "metadata": {
        "id": "XGVagvU-zTNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_results = (\n",
        "    pd.DataFrame(cv_results[\"metrics\"])\n",
        "    .drop(columns=\"i_split\")\n",
        "    .groupby([\"model\"], sort=False)\n",
        "    .agg([\"mean\"])\n",
        ")\n",
        "pivot_results"
      ],
      "metadata": {
        "id": "m4v2muOFzW14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "two_stage_catboost_ranker.get_train_with_targets_for_reranker(dataset)"
      ],
      "metadata": {
        "id": "CmBWQ1rxYeyx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}