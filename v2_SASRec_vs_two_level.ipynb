{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1N9ELVwC7Vf72jlYlO8wDBi3-wQ_DRImu",
      "authorship_tag": "ABX9TyMeu1KzDlAZqfjLo52tLAiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/budennovsk/AuthorBooksComments/blob/master/v2_SASRec_vs_two_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install implicit catboost rectools[lightfm] replay-rec==0.21.2rc0 #replay-rec\n",
        "# !pip -q uninstall -y pyspark\n",
        "# !pip -q install \"pyspark==3.4.3\"\n",
        "import sys\n",
        "import pyspark\n",
        "print(\"python:\", sys.version)\n",
        "print(\"pyspark:\", pyspark.__version__)"
      ],
      "metadata": {
        "id": "DU66BHTGShrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def _ensure_datetime(df: pd.DataFrame, col: str = \"datetime\") -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    if col in out.columns and not np.issubdtype(out[col].dtype, np.datetime64):\n",
        "        out[col] = pd.to_datetime(out[col], errors=\"coerce\")\n",
        "    return out\n",
        "\n",
        "\n",
        "def _safe_log1p(s: pd.Series) -> pd.Series:\n",
        "    return np.log1p(s.astype(\"float64\"))\n",
        "\n",
        "\n",
        "def _add_quantiles(features: pd.DataFrame, group: pd.core.groupby.generic.SeriesGroupBy, prefix: str):\n",
        "    # добавляет q05/q50/q95 как отдельные колонки\n",
        "    q = group.quantile([0.05, 0.5, 0.95]).unstack(level=-1)\n",
        "    q.columns = [f\"{prefix}_q{int(p*100):02d}\" for p in q.columns]\n",
        "    return features.join(q, how=\"left\")\n",
        "\n",
        "\n",
        "def add_pairwise_features(\n",
        "    interactions: pd.DataFrame,\n",
        "    user_col: str = \"user_id\",\n",
        "    item_col: str = \"item_id\",\n",
        "    dt_col: str = \"datetime\",\n",
        "    relevance_col: str = \"watched_pct\",\n",
        "    cold_user_threshold: int = 1,\n",
        "    cold_item_threshold: int = 1,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Вход: interactions с колонками [user_id, item_id, datetime, total_dur, watched_pct, weight]\n",
        "    Выход: тот же датафрейм + новые фичи (user- и item-агрегаты примержены обратно на строки взаимодействий).\n",
        "    \"\"\"\n",
        "    df = _ensure_datetime(interactions, dt_col)\n",
        "\n",
        "    # ---------- Кол-во взаимодействий ----------\n",
        "    u_cnt = df.groupby(user_col).size().rename(\"user_interactions_cnt\")\n",
        "    i_cnt = df.groupby(item_col).size().rename(\"item_interactions_cnt\")\n",
        "\n",
        "    df = df.join(u_cnt, on=user_col).join(i_cnt, on=item_col)\n",
        "\n",
        "    df[\"user_log_interactions_cnt\"] = _safe_log1p(df[\"user_interactions_cnt\"])\n",
        "    df[\"item_log_interactions_cnt\"] = _safe_log1p(df[\"item_interactions_cnt\"])\n",
        "\n",
        "    # ---------- Средний log(кол-ва) \"по соседям\" ----------\n",
        "    # item_avg_user_log: средний user_log_interactions_cnt среди пользователей, которые взаимодействовали с item\n",
        "    item_avg_user_log = (\n",
        "        df.groupby(item_col)[\"user_log_interactions_cnt\"]\n",
        "        .mean()\n",
        "        .rename(\"item_avg_user_log_interactions_cnt\")\n",
        "    )\n",
        "    # user_avg_item_log: средний item_log_interactions_cnt среди айтемов, с которыми взаимодействовал user\n",
        "    user_avg_item_log = (\n",
        "        df.groupby(user_col)[\"item_log_interactions_cnt\"]\n",
        "        .mean()\n",
        "        .rename(\"user_avg_item_log_interactions_cnt\")\n",
        "    )\n",
        "\n",
        "    df = df.join(item_avg_user_log, on=item_col).join(user_avg_item_log, on=user_col)\n",
        "\n",
        "    # разница (как ты просил) между \"своим\" кол-вом и средним (по соседям)\n",
        "    df[\"user_diff_cnt_vs_avg_items\"] = df[\"user_interactions_cnt\"] - df[\"user_avg_item_log_interactions_cnt\"].fillna(0.0)\n",
        "    df[\"item_diff_cnt_vs_avg_users\"] = df[\"item_interactions_cnt\"] - df[\"item_avg_user_log_interactions_cnt\"].fillna(0.0)\n",
        "\n",
        "    # cold flags\n",
        "    df[\"user_cold_flag\"] = (df[\"user_interactions_cnt\"] <= cold_user_threshold).astype(\"int8\")\n",
        "    df[\"item_cold_flag\"] = (df[\"item_interactions_cnt\"] <= cold_item_threshold).astype(\"int8\")\n",
        "\n",
        "    # ---------- Timestamp-based ----------\n",
        "    if dt_col in df.columns and np.issubdtype(df[dt_col].dtype, np.datetime64):\n",
        "        # min/max ts\n",
        "        u_min = df.groupby(user_col)[dt_col].min().rename(\"user_min_ts\")\n",
        "        u_max = df.groupby(user_col)[dt_col].max().rename(\"user_max_ts\")\n",
        "        i_min = df.groupby(item_col)[dt_col].min().rename(\"item_min_ts\")\n",
        "        i_max = df.groupby(item_col)[dt_col].max().rename(\"item_max_ts\")\n",
        "\n",
        "        df = (\n",
        "            df.join(u_min, on=user_col)\n",
        "              .join(u_max, on=user_col)\n",
        "              .join(i_min, on=item_col)\n",
        "              .join(i_max, on=item_col)\n",
        "        )\n",
        "\n",
        "        # history length\n",
        "        df[\"user_history_timedelta\"] = df[\"user_max_ts\"] - df[\"user_min_ts\"]\n",
        "        df[\"item_history_timedelta\"] = df[\"item_max_ts\"] - df[\"item_min_ts\"]\n",
        "\n",
        "        df[\"user_history_days\"] = df[\"user_history_timedelta\"].dt.total_seconds() / 86400.0\n",
        "        df[\"item_history_days\"] = df[\"item_history_timedelta\"].dt.total_seconds() / 86400.0\n",
        "\n",
        "        # log number of interaction days: кол-во уникальных дней с взаимодействиями\n",
        "        tmp_date = df[dt_col].dt.floor(\"D\")\n",
        "        u_days = tmp_date.groupby(df[user_col]).nunique().rename(\"user_interaction_days_cnt\")\n",
        "        i_days = tmp_date.groupby(df[item_col]).nunique().rename(\"item_interaction_days_cnt\")\n",
        "\n",
        "        df = df.join(u_days, on=user_col).join(i_days, on=item_col)\n",
        "\n",
        "        df[\"user_log_interaction_days_cnt\"] = _safe_log1p(df[\"user_interaction_days_cnt\"])\n",
        "        df[\"item_log_interaction_days_cnt\"] = _safe_log1p(df[\"item_interaction_days_cnt\"])\n",
        "\n",
        "        # days since last interaction (относительно последней даты в логе)\n",
        "        last_log_ts = df[dt_col].max()\n",
        "        df[\"user_days_since_last_interaction\"] = (last_log_ts - df[\"user_max_ts\"]).dt.total_seconds() / 86400.0\n",
        "        df[\"item_days_since_last_interaction\"] = (last_log_ts - df[\"item_max_ts\"]).dt.total_seconds() / 86400.0\n",
        "\n",
        "    # ---------- Relevance-based (по watched_pct) ----------\n",
        "    if relevance_col in df.columns:\n",
        "        # user relevance stats\n",
        "        g_u = df.groupby(user_col)[relevance_col]\n",
        "        u_mean = g_u.mean().rename(\"user_rel_mean\")\n",
        "        u_std = g_u.std(ddof=0).rename(\"user_rel_std\")\n",
        "\n",
        "        # item relevance stats\n",
        "        g_i = df.groupby(item_col)[relevance_col]\n",
        "        i_mean = g_i.mean().rename(\"item_rel_mean\")\n",
        "        i_std = g_i.std(ddof=0).rename(\"item_rel_std\")\n",
        "\n",
        "        df = (\n",
        "            df.join(u_mean, on=user_col)\n",
        "              .join(u_std, on=user_col)\n",
        "              .join(i_mean, on=item_col)\n",
        "              .join(i_std, on=item_col)\n",
        "        )\n",
        "\n",
        "        # quantiles\n",
        "        # (делаем через отдельные таблицы, чтобы не раздувать расчёты на каждой строке)\n",
        "        u_q = g_u.quantile([0.05, 0.5, 0.95]).unstack(level=-1)\n",
        "        u_q.columns = [\"user_rel_q05\", \"user_rel_q50\", \"user_rel_q95\"]\n",
        "\n",
        "        i_q = g_i.quantile([0.05, 0.5, 0.95]).unstack(level=-1)\n",
        "        i_q.columns = [\"item_rel_q05\", \"item_rel_q50\", \"item_rel_q95\"]\n",
        "\n",
        "        df = df.join(u_q, on=user_col).join(i_q, on=item_col)\n",
        "\n",
        "        # ---------- Abnormality (простой вариант через KL divergence p_u(item) || q(item)) ----------\n",
        "        # q(item): доля взаимодействий с item во всём логе\n",
        "        item_pop = (df.groupby(item_col).size() / len(df)).rename(\"q_item_pop\")\n",
        "\n",
        "        # p_u(item): доля взаимодействий пользователя с item\n",
        "        ui_cnt = df.groupby([user_col, item_col]).size().rename(\"ui_cnt\").reset_index()\n",
        "        u_total = ui_cnt.groupby(user_col)[\"ui_cnt\"].sum().rename(\"u_total\").reset_index()\n",
        "\n",
        "        ui_cnt = ui_cnt.merge(u_total, on=user_col, how=\"left\")\n",
        "        ui_cnt[\"p_ui\"] = ui_cnt[\"ui_cnt\"] / ui_cnt[\"u_total\"]\n",
        "\n",
        "        ui_cnt = ui_cnt.merge(item_pop.reset_index(), on=item_col, how=\"left\")\n",
        "\n",
        "        eps = 1e-12\n",
        "        ui_cnt[\"kl_term\"] = ui_cnt[\"p_ui\"] * np.log((ui_cnt[\"p_ui\"] + eps) / (ui_cnt[\"q_item_pop\"] + eps))\n",
        "        user_abn = ui_cnt.groupby(user_col)[\"kl_term\"].sum().rename(\"user_pref_abnormality_kl\")\n",
        "\n",
        "        df = df.join(user_abn, on=user_col)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "7l7QgkUUO8bE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d46GbIK-Q5y8"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import typing as tp\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from implicit.nearest_neighbours import CosineRecommender\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from catboost import CatBoostClassifier, CatBoostRanker\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier, LGBMRanker\n",
        "    LGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    warnings.warn(\"lightgbm is not installed. Some parts of the notebook will be skipped.\")\n",
        "    LGBM_AVAILABLE = False\n",
        "from rectools.dataset import Interactions\n",
        "from lightfm import LightFM\n",
        "from rectools import Columns\n",
        "from rectools.dataset import Dataset\n",
        "from rectools.metrics import Precision, Recall, MeanInvUserFreq, Serendipity,MAP,NDCG,HitRate\n",
        "from rectools.models import (\n",
        "    ImplicitALSWrapperModel,\n",
        "    ImplicitBPRWrapperModel,\n",
        "    LightFMWrapperModel,\n",
        "    PureSVDModel,\n",
        "    ImplicitItemKNNWrapperModel,\n",
        "    EASEModel,\n",
        "    PopularModel)\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from implicit.bpr import BayesianPersonalizedRanking\n",
        "from implicit.nearest_neighbours import CosineRecommender\n",
        "from rectools.models.base import ExternalIds\n",
        "from rectools.models.ranking import (\n",
        "    CandidateRankingModel,\n",
        "    CandidateGenerator,\n",
        "    Reranker,\n",
        "    CatBoostReranker,\n",
        "    CandidateFeatureCollector,\n",
        "    PerUserNegativeSampler\n",
        ")\n",
        "from rectools.model_selection import cross_validate, TimeRangeSplitter,LastNSplitter,Splitter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_users = '/content/drive/MyDrive/Colab Notebooks/Симбирсофт/recsys/dataset/data_original/users.csv'\n",
        "path_items = '/content/drive/MyDrive/Colab Notebooks/Симбирсофт/recsys/dataset/data_original/items.csv'\n",
        "path_interactions = '/content/drive/MyDrive/Colab Notebooks/Симбирсофт/recsys/dataset/data_original/interactions.csv'\n",
        "\n",
        "\n",
        "users = pd.read_csv(path_users)\n",
        "items = pd.read_csv(path_items)\n",
        "interactions = (\n",
        "    pd.read_csv(path_interactions, parse_dates=[\"last_watch_dt\"])\n",
        "    .rename(columns={\"last_watch_dt\": Columns.Datetime})\n",
        ")\n",
        "interactions = interactions.head(1000000)\n",
        "users_clise = users[users['user_id'].isin(interactions['user_id'].unique())]\n",
        "interactions[\"weight\"] = 1\n",
        "dataset = Dataset.construct(interactions)\n",
        "RANDOM_STATE = 32\n",
        "# dataset"
      ],
      "metadata": {
        "id": "yYk0XhrxWWxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from replay.preprocessing.history_based_fp import LogStatFeaturesProcessor\n",
        "# from replay.utils.session_handler import get_spark_session, State\n",
        "# from replay.experimental.preprocessing.data_preparator import DataPreparator, Indexer\n",
        "# spark = State().session\n",
        "# spark.sparkContext.setLogLevel('ERROR')\n",
        "# dp = DataPreparator()\n",
        "# log = dp.transform(data=interactions,\n",
        "#                   columns_mapping={\n",
        "#                       \"user_id\": \"user_id\",\n",
        "#                       \"item_id\":  \"item_id\",\n",
        "#                       \"relevance\": \"watched_pct\",\n",
        "#                       \"timestamp\": \"datetime\"\n",
        "#                   })\n",
        "\n",
        "# log.show(2)"
      ],
      "metadata": {
        "id": "oHkq9DDdxfp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# indexer = Indexer(user_col='user_id', item_col='item_id')\n",
        "# indexer.fit(users=log.select('user_id'),\n",
        "#             items=log.select('item_id'))\n",
        "# log = indexer.transform(df=log)\n",
        "# log.show(2)"
      ],
      "metadata": {
        "id": "AFzFWc23-CU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from replay.preprocessing.history_based_fp import LogStatFeaturesProcessor\n",
        "# lf = LogStatFeaturesProcessor()\n",
        "# lf.fit(log)\n",
        "# log_trsfrm = lf.transform(log)\n",
        "# log_trsfrm.show(1, vertical=True)"
      ],
      "metadata": {
        "id": "Kv5xCnVg-CXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_trsfrm = lf.transform(log)"
      ],
      "metadata": {
        "id": "9bOcftl_P7BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_trsfrm.show(1, vertical=True)"
      ],
      "metadata": {
        "id": "l63c31Rl-CZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_pd = log_trsfrm.toPandas()\n",
        "# rename_map = {\n",
        "#     \"user_idx\": \"user_id\",\n",
        "#     \"item_idx\": \"item_id\",\n",
        "#     \"timestamp\": \"datetime\",\n",
        "#     \"relevance\": \"watched_pct\"}\n",
        "\n",
        "# log_pd = log_pd.rename(columns=rename_map)\n",
        "# # log_pd  = Interactions.construct(log_pd)\n",
        "# log_pd"
      ],
      "metadata": {
        "id": "_MtRA6q7iq6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interactions_fe = add_pairwise_features(interactions)"
      ],
      "metadata": {
        "id": "pT_gCfdYO54I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_pd[log_pd['total_dur']==57581].T"
      ],
      "metadata": {
        "id": "SsH-Q-olU2QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interactions_fe[interactions_fe['total_dur']==57581].T"
      ],
      "metadata": {
        "id": "b2ltIIY3U3Y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# log_pd[['item_id', 'user_id', 'datetime', 'total_dur', 'watched_pct', 'weight',\n",
        "#        'u_log_num_interact', 'u_log_interact_days_count',\n",
        "#        'u_min_interact_date', 'u_max_interact_date', 'u_std', 'u_mean',\n",
        "#        'u_quantile_05', 'u_quantile_5', 'u_quantile_95',\n",
        "#        'u_history_length_days', 'u_last_interaction_gap_days', 'abnormality',\n",
        "#        'abnormalityCR', 'u_mean_i_log_num_interact', 'i_log_num_interact',\n",
        "#        'i_log_interact_days_count', 'i_min_interact_date',\n",
        "#        'i_max_interact_date', 'i_std', 'i_mean', 'i_quantile_05',\n",
        "#        'i_quantile_5', 'i_quantile_95', 'i_history_length_days',\n",
        "#        'i_last_interaction_gap_days', 'i_mean_u_log_num_interact',\n",
        "#        'na_u_log_features', 'na_i_log_features', 'u_i_log_num_interact_diff',\n",
        "#        'i_u_log_num_interact_diff']]"
      ],
      "metadata": {
        "id": "3z7N2M9dZEwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare first stage models. They will be used to generate candidates for reranking\n",
        "first_stage = [\n",
        "    CandidateGenerator(PopularModel(), num_candidates=100, keep_ranks=True, keep_scores=True),\n",
        "    CandidateGenerator(\n",
        "        ImplicitItemKNNWrapperModel(CosineRecommender()),\n",
        "        num_candidates=100,\n",
        "        keep_ranks=True,\n",
        "        keep_scores=True\n",
        "    ),\n",
        "    CandidateGenerator(\n",
        "        ImplicitALSWrapperModel(\n",
        "          AlternatingLeastSquares(\n",
        "            factors=10,  # latent embeddings size\n",
        "            regularization=0.1,\n",
        "            iterations=10,\n",
        "            alpha=50,  # confidence multiplier for non-zero entries in interactions\n",
        "            random_state=RANDOM_STATE)),\n",
        "    num_candidates=100, keep_ranks=True, keep_scores=True),\n",
        "    CandidateGenerator(\n",
        "        LightFMWrapperModel(\n",
        "            LightFM(no_components=10,\n",
        "                    loss=\"bpr\",\n",
        "                    random_state=RANDOM_STATE)),\n",
        "    num_candidates=100, keep_ranks=True, keep_scores=True\n",
        ")\n",
        "]"
      ],
      "metadata": {
        "id": "PKNU4EvuaVfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Write custom feature collecting funcs for users, items and user/item pairs\n",
        "# class CustomFeatureCollector(CandidateFeatureCollector):\n",
        "\n",
        "#     def __init__(self, user_features_path: Path, user_cat_cols: tp.List[str]) -> None:\n",
        "#         self.user_features_path = user_features_path\n",
        "#         self.user_cat_cols = user_cat_cols\n",
        "\n",
        "#     # your any helper functions for working with loaded data\n",
        "#     def _encode_cat_cols(self, df: pd.DataFrame, cols: tp.List[str]) -> pd.DataFrame:\n",
        "#         for col in cols:\n",
        "#             df[col] = df[col].astype(\"category\").cat.codes.astype(\"category\")\n",
        "#         return df\n",
        "\n",
        "#     def _get_user_features(\n",
        "#         self, users: ExternalIds, dataset: Dataset, fold_info: tp.Optional[tp.Dict[str, tp.Any]]\n",
        "#     ) -> pd.DataFrame:\n",
        "#         columns = self.user_cat_cols.copy()\n",
        "#         columns.append(Columns.User)\n",
        "#         user_features = users_clise[columns]\n",
        "\n",
        "#         users_without_features = pd.DataFrame(\n",
        "#             np.setdiff1d(dataset.user_id_map.external_ids, user_features[Columns.User].unique()),\n",
        "#             columns=[Columns.User]\n",
        "#         )\n",
        "#         user_features = pd.concat([user_features, users_without_features], axis=0)\n",
        "#         user_features = self._encode_cat_cols(user_features, self.user_cat_cols)\n",
        "#         print(user_features[user_features[Columns.User].isin(users)])\n",
        "#         return user_features[user_features[Columns.User].isin(users)]\n",
        "# # To transfer CatBoostRanker we use CatBoostReranker\n",
        "# splitter = TimeRangeSplitter(\"1D\", n_splits=1)\n",
        "# pool_kwargs = {\n",
        "#     \"cat_features\": [\"age\", \"income\", \"sex\"]\n",
        "# }\n",
        "\n",
        "# two_stage_catboost_ranker = CandidateRankingModel(\n",
        "#     candidate_generators=first_stage,\n",
        "#     splitter=splitter,\n",
        "#     reranker=CatBoostReranker(CatBoostRanker(verbose=False, random_state=RANDOM_STATE),pool_kwargs=pool_kwargs),#pool_kwargs=pool_kwargs\n",
        "#     sampler=PerUserNegativeSampler(n_negatives=3, random_state=RANDOM_STATE), # pass sampler to fix random_state\n",
        "#      feature_collector=CustomFeatureCollector(\n",
        "#         user_features_path=path_users,\n",
        "#         user_cat_cols=[\"age\", \"income\", \"sex\"],\n",
        "#     )\n",
        "# )\n",
        "# candidates = two_stage_catboost_ranker.get_train_with_targets_for_reranker(dataset) #log_pd dataset\n",
        "# # candidates"
      ],
      "metadata": {
        "id": "_voZv38ysOye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomFeatureCollector(CandidateFeatureCollector):\n",
        "\n",
        "    def __init__(self, log_df) -> None:\n",
        "        self.log_df = log_df\n",
        "\n",
        "\n",
        "    def _get_user_item_features(\n",
        "        self, useritem: pd.DataFrame, dataset: Dataset, fold_info: tp.Optional[tp.Dict[str, tp.Any]]\n",
        "    ) -> pd.DataFrame:\n",
        "\n",
        "        users_without_features = pd.DataFrame(\n",
        "            np.setdiff1d(dataset.user_id_map.external_ids, self.log_df['user_id'].unique()),\n",
        "            columns=['user_id']\n",
        "        )\n",
        "        # print('users_without_features',users_without_features) #здесь пустой датасет возрвщается потмоу что у меня внешние айдишники в dataset == self.log_df['user_id'].unique() сгенерированому датасету над признакми\n",
        "        # print(self.log_df)\n",
        "\n",
        "        test_data = self.log_df[self.log_df['user_id'].isin(useritem['user_id'].values)].fillna(0)\n",
        "        # print('return ', test_data)\n",
        "        # return test_data[['item_id','user_id','user_log_interactions_cnt']]\n",
        "        # return test_data[['item_id','user_id','user_log_interactions_cnt']]\n",
        "\n",
        "        return test_data[[i for i in test_data.columns if not i in [\"datetime\",\"user_min_ts\",\"user_max_ts\",\"item_min_ts\",\"item_max_ts\",\"user_history_timedelta\",\"item_history_timedelta\"]]]\n",
        "\n",
        "\n",
        "# To transfer CatBoostRanker we use CatBoostReranker\n",
        "splitter = TimeRangeSplitter(\"7D\", n_splits=1)\n",
        "\n",
        "\n",
        "two_stage_catboost_ranker = CandidateRankingModel(\n",
        "    candidate_generators=first_stage,\n",
        "    splitter=splitter,\n",
        "    reranker=CatBoostReranker(CatBoostRanker(verbose=False, random_state=RANDOM_STATE)),#pool_kwargs=pool_kwargs\n",
        "    sampler=PerUserNegativeSampler(n_negatives=3, random_state=RANDOM_STATE), # pass sampler to fix random_state\n",
        "    # feature_collector=CandidateFeatureCollector(),\n",
        "    feature_collector=CustomFeatureCollector(log_df= add_pairwise_features(interactions)),\n",
        "    verbose=1\n",
        ")\n",
        "# candidates = two_stage_catboost_ranker.get_train_with_targets_for_reranker(dataset) #log_pd dataset\n",
        "# candidates.head(5)"
      ],
      "metadata": {
        "id": "LO8CtVoFieZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # To transfer CatBoostRanker we use CatBoostReranker\n",
        "# splitter = TimeRangeSplitter(\"1D\", n_splits=1)\n",
        "\n",
        "\n",
        "# two_stage_catboost_ranker = CandidateRankingModel(\n",
        "#     candidate_generators=first_stage,\n",
        "#     splitter=splitter,\n",
        "#     reranker=CatBoostReranker(CatBoostRanker(verbose=False, random_state=RANDOM_STATE)),#pool_kwargs=pool_kwargs\n",
        "#     sampler=PerUserNegativeSampler(n_negatives=3, random_state=RANDOM_STATE), # pass sampler to fix random_state\n",
        "#     # feature_collector=CandidateFeatureCollector(),\n",
        "#     feature_collector=CustomFeatureCollector(log_df= add_pairwise_features(interactions)),\n",
        "#     verbose=1\n",
        "# )\n",
        "# candidates = two_stage_catboost_ranker.get_train_with_targets_for_reranker(dataset) #log_pd dataset\n",
        "# candidates"
      ],
      "metadata": {
        "id": "VDMM1UNWXY5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trtt = add_pairwise_features(interactions)\n",
        "# trtt[trtt['user_id']==684368]"
      ],
      "metadata": {
        "id": "FoHmpCFLYxtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# candi = two_stage_catboost_ranker.split_to_history_dataset_and_train_targets(dataset,splitter)\n",
        "# candi"
      ],
      "metadata": {
        "id": "cd5JDuXLVuvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interactions[interactions['user_id']==1014363]"
      ],
      "metadata": {
        "id": "q3SgkTfFl_CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interactions"
      ],
      "metadata": {
        "id": "t5PLnoDFFOtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactions[interactions['datetime']=='2021-08-22']"
      ],
      "metadata": {
        "id": "alsZjBtxXZGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactions[interactions['user_id'].isin([966733,424980,761394,1014363,684368])] #966733  1014363"
      ],
      "metadata": {
        "id": "1WyzKl7CXZIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "items[items['item_id']==913] #931 1267"
      ],
      "metadata": {
        "id": "H3CE-eEAcxd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users[users['user_id']==684368] #684368 1014363"
      ],
      "metadata": {
        "id": "-axbdQRCa8yJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-UiARsra809"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TyzLKTtyXZLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lVbFPmexXZN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # To transfer CatBoostRanker we use CatBoostReranker\n",
        "# splitter = TimeRangeSplitter(\"7D\", n_splits=1)\n",
        "\n",
        "# # Categorical features are definitely transferred to the pool_kwargs\n",
        "# pool_kwargs = {\n",
        "#     \"cat_features\": [\"age\", \"income\", \"sex\"]\n",
        "# }\n",
        "\n",
        "# two_stage_catboost_ranker = CandidateRankingModel(\n",
        "#     candidate_generators=first_stage,\n",
        "#     splitter=splitter,\n",
        "#     reranker=CatBoostReranker(CatBoostRanker(verbose=False, random_state=RANDOM_STATE), pool_kwargs=pool_kwargs),\n",
        "#     sampler=PerUserNegativeSampler(n_negatives=3, random_state=RANDOM_STATE), # pass sampler to fix random_state\n",
        "#     # feature_collector=CandidateFeatureCollector(),\n",
        "#     feature_collector=CustomFeatureCollector(user_features_path=path_users, user_cat_cols=[\"age\", \"income\", \"sex\"] ),\n",
        "\n",
        "# )"
      ],
      "metadata": {
        "id": "2jOSxTKahrpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_users = dataset.user_id_map.external_ids\n",
        "users_to_recommend = all_users[:100]\n",
        "\n",
        "two_stage_catboost_ranker.fit(dataset)\n",
        "reco_catboost_ranker = two_stage_catboost_ranker.recommend(\n",
        "    users=users_to_recommend,\n",
        "    dataset=dataset,\n",
        "    k=10,\n",
        "    filter_viewed=True\n",
        ")\n",
        "reco_catboost_ranker.head(5)"
      ],
      "metadata": {
        "id": "vfCXY0LwuIxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take few models to compare\n",
        "models = {\n",
        "    \"popular\": PopularModel(),\n",
        "    \"cosine_knn\": ImplicitItemKNNWrapperModel(CosineRecommender()),\n",
        "    'iALS':ImplicitALSWrapperModel(\n",
        "          AlternatingLeastSquares(\n",
        "            factors=10,  # latent embeddings size\n",
        "            regularization=0.1,\n",
        "            iterations=10,\n",
        "            alpha=50,  # confidence multiplier for non-zero entries in interactions\n",
        "            random_state=RANDOM_STATE)),\n",
        "    'LightFM':LightFMWrapperModel(\n",
        "            LightFM(no_components=10,\n",
        "                    loss=\"bpr\",\n",
        "                    random_state=RANDOM_STATE)),\n",
        "    \"two_stage_catboost_ranker\": two_stage_catboost_ranker,\n",
        "}\n",
        "\n",
        "# We will calculate several classic (precision@k and recall@k) and \"beyond accuracy\" metrics\n",
        "metrics = {\n",
        "    \"prec@10\": Precision(k=10),\n",
        "    \"recall@10\": Recall(k=10),\n",
        "    \"novelty@10\": MeanInvUserFreq(k=10),\n",
        "    \"serendipity@10\": Serendipity(k=10),\n",
        "    \"MAP@10\": MAP(k=10),\n",
        "    \"NDCG@10\": NDCG(k=10),\n",
        "    \"HitRate@10\": HitRate(k=10)\n",
        "}\n",
        "\n",
        "K_RECS = 10"
      ],
      "metadata": {
        "id": "mZKMvPCNxqGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_results = cross_validate(\n",
        "    dataset=dataset,\n",
        "    splitter=splitter,\n",
        "    models=models,\n",
        "    metrics=metrics,\n",
        "    k=K_RECS,\n",
        "    filter_viewed=True,\n",
        ")"
      ],
      "metadata": {
        "id": "XGVagvU-zTNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_results = (\n",
        "    pd.DataFrame(cv_results[\"metrics\"])\n",
        "    .drop(columns=\"i_split\")\n",
        "    .groupby([\"model\"], sort=False)\n",
        "    .agg([\"mean\"])\n",
        ")\n",
        "pivot_results"
      ],
      "metadata": {
        "id": "m4v2muOFzW14"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}